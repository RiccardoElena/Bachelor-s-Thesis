\chapter{First Order Logic}\label{chap:first-order-logic}

%\inlineminitoc{}

\noindent First-order logic (FOL), or predicate logic, is a formal system used in mathematics, philosophy, linguistics, and computer science to express statements about objects and their relationships.
It uses \bold{terms} for the representation of objects in the domain of discourse and \bold{predicates} to express properties of these objects or relationships between them.
Moreover, FOL allows for the use of \bold{quantifiers} to make statements that apply to all objects (universal quantification) or some objects (existential quantification) in the domain, and \bold{logical connectives} to construct more complex statements.

One well-known informal statement, \dquote{All humans are mortal} for example, can be expressed in FOL as:
\begin{equation*}
    \forall x \left( \text{H}(x) \implies \text{M}(x) \right)
\end{equation*}

Syntactically speaking, this sentence is composed of:
\begin{itemize}
    \item a \bold{quantifier} \(\forall\)
    \item a \bold{variable} \(x\) which ranges over the whole domain of discourse (possibly every object in the universe)
    \item a \bold{unary predicate} \(\text{H}\) applied to \(x\)
    \item a \bold{unary predicate} \(\text{M}\) applied to \(x\)
    \item a \bold{logical connective} \(\implies\)
\end{itemize}

Upon this syntactic structure, we can build a semantic interpretation, which assigns meaning to the symbols used in the sentence.
In this case, we can interpret \(\text{H}(x)\) as \dquote{\(x\) is a human} and \(\text{M}(x)\) as \dquote{\(x\) is mortal}.
Moreover, the universal quantifier \(\forall\) indicates that the statement applies to all objects in the domain of discourse.
The connective \(\implies\) is the \emph{material implication} of \emph{propositional logic}, indicating that if the first part is true, then the second part must also be true.

As we can see, using FOL we can divide the statement into its syntactic and semantic components, which allows us to reason about its structure separately from its meaning.
This separation permits us to generalize the reasoning process to other syntactically similar statements and to deduce new statements solely from their structure.
This makes FOL particularly suitable for computational treatment: automated systems can manipulate the syntactic form of statements while guaranteeing that these manipulations reflect valid semantic consequences in all interpretations — a property known as \emph{soundness}~\cite{enderton2001}.
As we will see later, this clear separation is crucial for the development of automated reasoning systems, such as the Vampire theorem prover, because it allows us to convert the reasoning task into a syntactic manipulation task, treating statements as symbolic strings with a specific syntax, without the need to explicitly encode the meaning of each statement.

In order to preserve the truth value of statements during this manipulation process, we cannot rely on the syntactic structure alone; we must define a set of rules that act only on the syntactic structure while ensuring truth preservation.
This set of rules is called \bold{inference rules}, and they allow us to derive new statements from existing ones without altering their truth value.
A full exploration of these rules, which form the core of automated reasoning, is reserved for a later chapter. However, a simple example is the well-known rule of \emph{Modus Ponens}, which states that if we know that both \(A\) and \(A\implies B\) are true, then we can conclude \(B\). This illustrates how a new truth can be derived purely from the syntactic form of existing statements.

\section{From Terms to Sentences}\label{sec:from-terms-to-sentences}

First and foremost, we need to rigorously define the structure of first-order logic.
The most basic building blocks of FOL are \bold{terms}, which represent objects in the domain of discourse.
Those are variables (\(x,y,z\)), functions (\(f,g,h\)) and constants (\(a,b,c\)), sometimes viewed as function symbols with arity 0.
Variables are placeholders that can represent any object in the domain, while constants refer to specific objects. Functions map objects to other objects, allowing for more complex expressions.

Terms alone do not constitute statements in FOL, as they lack an associated \emph{truth value}.
The main focus of FOL is on \bold{predicates} (\(P,Q,R\)) ---so the name \emph{predicate logic}---, which are used to express properties of objects or relationships between them.
Examples of predicates include \(\text{H}(x)\) for \dquote{is a human} or \(\text{M}(x)\) for \dquote{is mortal}.
Predicates are applied to terms to form \bold{atomic formulae} (\(A,B\)), the basic units of meaning in FOL\@. They serve as the building blocks of more complex and structured statements, called \bold{formulae} \(\phi, \psi\), obtained by combining atomic formulae using logical connectives and quantifiers.

Solely, not even atomic formulae always bear a truth value, as they can contain \emph{free variables}, namely variables that are not bound by a quantifier. These variables do not refer to any specific object in the domain, and thus the truth value of the atomic formula depends on the interpretation of these variables.
This ambiguity can be illustrated with natural language; for instance, the statement \dquote{X is a human} does not clearly state \emph{which} object \(X\) has to be a human to make the statement true.
The quantifiers resolve this ambiguity by specifying the scope of the variable \(X\). Indeed, the statement \dquote{For all X, X is a human} clearly has a truth value, as it asserts something about every object, whether it is true or not.
Only (atomic) formulae with all variables bound can be said to have a definite truth value, and those are called (\bold{atomic}) \bold{sentences}.

Last but not least, to combine (atomic) formulae/sentences into more complex expressions, we can use logical connectives. Those are symbols that applied to one or more formulae yield a new formula, whose truth value depends on the truth values of the original formulae and on the semantics of the connective.
In particular, the principal connectives used in FOL are:

\begin{itemize}
  \item \bold{Negation} (\(\neg\)): This connective takes a single formula and \bold{inverts} its truth value.
  \item \bold{Conjunction} (\(\land\)): This connective combines two formulae and is \bold{true} if both are true.
  \item \bold{Disjunction} (\(\lor\)): This connective combines two formulae and is \bold{true} if at least one is true.
  \item \bold{Implication} (\(\implies\)): This connective expresses a conditional relationship between two formulae. The new formula is \bold{false} only if the first formula is true and the second formula is false.
  \item \bold{Biconditional} (\(\iff\)): This connective expresses an equivalence between two formulae. The new formula is \bold{true} if both formulae have the same truth value.
\end{itemize}

Commonly, we refer as \emph{literals} to atomic formulae or their negations.
\subsection{Syntax}\label{subsec:syntax}

The syntax of FOL formulae can be formalized with a \emph{context free grammar} (CFG).
In particular, being \(\mathcal{X}\) the set of all variables, \(\mathcal{C}\) the set of all constants, \(\mathcal{F}\) the set of all function symbols, \(\mathcal{P}\) the set of all predicate symbols, we can define the grammar as follows:

\begin{equation}
  FOL = \left( V , \Sigma, \phi \in V, R\right)
\end{equation}
where:
\begin{itemize}
  \item \(V = \{\tau, \alpha, \phi\}\)
  \item \(\Sigma = \mathcal{X} \cup \mathcal{C} \cup \mathcal{F} \cup \mathcal{P} \cup \{\forall, \exists, \land, \lor, \neg, \implies, \iff, \left(,\right), \top, \bot\}\)
   \item \(R\) = \begin{flalign}
    \begin{aligned}
      \tau \rightarrow  \ms  &x \in \mathcal{X}  \ms | \ms  
                        c \in \mathcal{C}  \ms | \ms  
                        f(\tau,\ldots,\tau) \in \mathcal{F} \\
      \alpha \rightarrow  \ms  &P(\tau,\ldots,\tau), P \in \mathcal{P} \\
      \phi \rightarrow  \ms  &\alpha  \ms | \ms  \top  \ms | \ms  \bot  \ms | \ms  
       \neg\phi  \ms |
       \left(\phi\land\phi\right) |
       \left(\phi\lor\phi\right) |
       \left(\phi\implies\phi\right) |
       \left(\phi\iff\phi\right) |  \ms 
       \forall x\left(\phi\right)  \ms | \ms 
       \exists x\left(\phi\right)
    \end{aligned} &&
  \end{flalign}
\end{itemize}

The language generated by this grammar is the set of all \emph{well-formed formulae} in FOL\@.
Every string of the so generated language can also be represented with a \emph{syntactic tree}, which is a tree representation of the syntactic structure of the formula, highlighting the hierarchical relationships between its components.
An example of a syntactic tree for the formula \(\forall x \left(\text{H}(x) \implies \text{M}(x)\right)\) is shown in Figure~\ref{fig:syntactic_tree}.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[syntree]
        \node[quantifier] {{\(\forall x\)}}
            child { node[connective] {\(\implies\)} 
              child { node[literal] {\(\text{H}(x)\)} }
              child { node[literal] {\(\text{M}(x)\)} }
            };
    \end{tikzpicture}
    \caption{Syntactic tree for \(\forall x (\text{H}(x) \implies \text{M}(x))\)}\label{fig:syntactic_tree}
\end{figure}

In this representation, it is possible to omit the parentheses around the subformulae, as the tree structure already encodes the necessary grouping.
For the sake of brevity, in formulae tree representations, atoms will be represented collapsed into single nodes, as shown in the example above.
Moreover, is possible to isolate a \emph{subgrammar} by considering only the rules that generate atomic formulae (excluding so the ones deriving from \(\phi\)). Doing this easily allows to define the concept of \bold{subterm} as a \emph{proper subtree} of the syntactic tree that represents an atomic formula.
An example of such a tree is shown in Figure~\ref{fig:subterm_tree} for the atomic formula \(P(f(x, g(y)))\).
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[syntree]
        \node[literal] {\(P\)}
            child { node[literal] {\(f\)}
              child { node[literal] {\(x\)} }
              child { node[literal] {\(g\)}
                child { node[literal] {\(y\)} }
              }
            };
    \end{tikzpicture}
    \caption{Syntactic tree for the atomic formula \(P(f(x, g(y)))\) showing nested function structure}\label{fig:subterm_tree}
\end{figure}

If we consider multiple atomic formulae, is possible to represent them as a \bold{forest} of syntactic trees, where each tree represents an atomic formula.

\begin{figure}[H]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}[syntree]
            \node[literal] {\(P\)}
                child { node[literal] {\(f\)}
                  child { node[literal] {\(x\)} }
                  child { node[literal] {\(g\)}
                    child { node[literal] {\(y\)} }
                  }
                };
        \end{tikzpicture}
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}[syntree]
            \node[literal] {\(\neg P\)}
                  child { node[literal] {\(f\)}
                    child { node[literal] {\(x\)} }
                    child { node[literal] {\(g\)}
                      child { node[literal] {\(y\)} }
                    }
                  };
        \end{tikzpicture}
    \end{minipage}
    \caption{Forest of the atomic formulae \(P(f(x, g(y)))\) and \(\neg P(f(x, g(y)))\)}\label{fig:subterm_forest}
\end{figure}


In practice, it is often more convenient to represent these structures as Directed Acyclic Graphs (DAGs) rather than trees. This is because DAGs allow for the sharing of subterms, which can lead to more compact representations and easier manipulation of the formulae.
DAGs can be particularly useful in automated reasoning and theorem proving, where the same subterm may appear in multiple places within a formula. By representing the formula as a DAG, we can avoid redundant copies of the subterms and simplify the reasoning process.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[syntree]
        % Livello delle variabili (condivise)
        \node[literal] (x) at (0,0) {\(x\)};
        \node[literal] (y) at (2,0) {\(y\)};
        
        % Livello delle funzioni (condivise)
        \node[literal] (g) at (2,1) {\(g\)};
        \node[literal] (f) at (1,2) {\(f\)};
        
        % Livello dei predicati (separati)
        \node[literal] (P_pos) at (-0.5,3) {\(P\)};
        \node[literal] (P_neg) at (2.5,3) {\(\neg P\)};
        
        % Connessioni condivise
        \draw[-latex, thick] (g) -- (y);
        \draw[-latex, thick] (f) -- (x);
        \draw[-latex, thick] (f) -- (g);

        % Connessioni per literal positivo
        \draw[-latex, thick] (P_pos) -- (f);

        % Connessioni per literal negativo
        \draw[-latex, thick] (P_neg) -- (f);


        % Etichette
        \node[left=0.3cm of P_pos, font=\footnotesize] {\(P(f(x, g(y)))\)};
        \node[right=0.3cm of P_neg, font=\footnotesize] {\(\neg P(f(x, g(y)))\)};
        
        % Evidenzia i nodi condivisi con colore diverso
        \node[literal] at (x) {\(x\)};
        \node[literal] at (y) {\(y\)};
        \node[literal] at (g) {\(g\)};
        \node[literal] at (f) {\(f\)};
    \end{tikzpicture}
    \caption{DAG representation showing shared subterms between \(P(f(x, g(y)))\) and \(\neg P(f(x, g(y)))\)}\label{fig:subterm_dag}
\end{figure}

\subsection{Semantics}\label{subsec:semantics}

To formalize semantics in FOL, we need to define the meaning of the symbols and the truth conditions for the formulae. This is typically done using a \emph{model}, which consists of a domain of discourse and an interpretation function that assigns meanings to the constants, functions, and predicates.

A \bold{model} for a FOL language is a pair \(M = (D, I)\), where:
\begin{itemize}
  \item \(D\) is a non-empty set, called the \bold{domain of discourse}.
  \item \(I\) is an interpretation function that assigns:
  \begin{itemize}
    \item Each constant \(c \in \mathcal{C}\) to an element \(I(c) \in D\).
    \item Each function symbol \(f \in \mathcal{F}\) to a function \(I(f): D^{n_f} \to D\), where \(n_f\) is the arity of \(f\).
    \item Each predicate symbol \(P \in \mathcal{P}\) to a relation \(I(P) \subseteq D^{n_P}\), where \(n_P\) is the arity of \(P\).
  \end{itemize}
\end{itemize}
The truth value of a formula \(\phi\) in a model \(M\) is determined by the interpretation function and the structure of the formula as follows:

\subsubsection{Variable Assignment}\label{subsubsec:variable_assignment}
Given a model \(M = (D, I)\), a \bold{variable assignment} (or \bold{valuation}) is a function \(\sigma: \mathcal{X} \to D\) that assigns each variable to an element of the domain. We denote by \(\sigma[x \mapsto d]\) the assignment that is identical to \(\sigma\) except that it maps variable \(x\) to element \(d \in D\).

\subsubsection{Term Evaluation}\label{subsubsec:term_evaluation}
The \bold{evaluation} of a term \(t\) in model \(M\) under assignment \(\sigma\), denoted \(\llbracket t \rrbracket_M^\sigma\), is defined recursively:
\begin{itemize}
  \item If \(t\) is a variable \(x\), then \(\llbracket x \rrbracket_M^\sigma = \sigma(x)\).
  \item If \(t\) is a constant \(c\), then \(\llbracket c \rrbracket_M^\sigma = I(c)\).
  \item If \(t = f(t_1, \ldots, t_n)\) where \(f\) is an \(n\)-ary function symbol, then 
        \[\llbracket f(t_1, \ldots, t_n) \rrbracket_M^\sigma = I(f)(\llbracket t_1 \rrbracket_M^\sigma, \ldots, \llbracket t_n \rrbracket_M^\sigma)\]
\end{itemize}

\subsubsection{Truth Conditions}\label{subsubsec:truth_conditions}
The \bold{satisfaction} of a formula \(\phi\) in model \(M\) under assignment \(\sigma\), denoted \(M, \sigma \models \phi\), is defined recursively:

\begin{itemize}
  \item \bold{Atomic formulae:}
  \begin{itemize}
    \item \(M, \sigma \models P(t_1, \ldots, t_n) \leftrightarrow (\llbracket t_1 \rrbracket_M^\sigma, \ldots, \llbracket t_n \rrbracket_M^\sigma) \in I(P)\)
    \item \(M, \sigma \models \top\) (always true)
    \item \(M, \sigma \not\models \bot\) (never true)
  \end{itemize}
  
  \item \bold{Logical connectives:}
  \begin{itemize}
    \item \(M, \sigma \models \neg \phi \leftrightarrow M, \sigma \not\models \phi\)
    \item \(M, \sigma \models \phi \land \psi \leftrightarrow M, \sigma \models \phi\) and \(M, \sigma \models \psi\)
    \item \(M, \sigma \models \phi \lor \psi \leftrightarrow M, \sigma \models \phi\) or \(M, \sigma \models \psi\)
    \item \(M, \sigma \models \phi \implies \psi \leftrightarrow M, \sigma \not\models \phi\) or \(M, \sigma \models \psi\)
    \item \(M, \sigma \models \phi \iff \psi \leftrightarrow \left(M, \sigma \models \phi \text{ and } M, \sigma \models \psi\right) \text{ or } \left(M, \sigma \not\models \phi \text{ and } M, \sigma \not\models \psi\right)\)
  \end{itemize}
  
  \item \bold{Quantifiers:}
  \begin{itemize}
    \item \(M, \sigma \models \forall x  \ms  \phi\) if and only if for all \(d \in D\), \(M, \sigma[x \mapsto d] \models \phi\)
    \item \(M, \sigma \models \exists x  \ms  \phi\) if and only if there exists some \(d \in D\) such that \(M, \sigma[x \mapsto d] \models \phi\)
  \end{itemize}
\end{itemize}

\subsubsection{Semantic Notions}\label{subsubsec:semantic_notions}
A formula \(\phi\) is:
\begin{itemize}
  \item \bold{Satisfiable} if there exists a model \(M\) and assignment \(\sigma\) such that \(M, \sigma \models \phi\).
  \item \bold{Valid} (sometimes called a \bold{tautology}) if for all models \(M\) and assignments \(\sigma\): \(M, \sigma \models \phi\). We write \(\models \phi\).
  \item \bold{Unsatisfiable} (or \bold{contradictory}) if for all models \(M\) and assignments \(\sigma\): \(M, \sigma \not\models \phi\).
\end{itemize}

For a set of formulae \(\Gamma\) and a formula \(\phi\), we say \(\Gamma\) \bold{semantically entails} \(\phi\), written \(\Gamma \models \phi\), if for all models \(M\) and assignments \(\sigma\):
  \[\text{if } M, \sigma \models \psi \text{ for all } \psi \in \Gamma, \text{ then } M, \sigma \models \phi\]
It is worth noticing that satisfiability and validity are closely related:
\begin{theorem}[Validity-Satisfiability Reduction]\label{thm:validity_satisfiability_reduction}
A formula \(\phi\) is valid if and only if \(\neg\phi\) is unsatisfiable. Formally:
\[\models \phi \iff M,\sigma\not\models\neg\phi \text{, for all } M,\sigma\]
\end{theorem}

\begin{proof}[Proof Sketch]
To prove this theorem, we need to show both directions of the equivalence.
  \begin{description}
    \item[(\(\Rightarrow\))] If \(\phi\) is valid, then \(M, \sigma \models \phi\) for all models \(M\) and assignments \(\sigma\). Therefore, assuming tertium non datur, \(M, \sigma \not\models \neg\phi\) for all \(M, \sigma\), making \(\neg\phi\) unsatisfiable.
    \item[(\(\Leftarrow\))] If \(\neg\phi\) is unsatisfiable, then there is no model \(M\) and assignment \(\sigma\) such that \(M, \sigma \models \neg\phi\). Therefore, for all \(M, \sigma\), again assuming tertium non datur we have \(M, \sigma \models \phi\), making \(\phi\) valid.
  \end{description}
\qed{}
\end{proof}


\section{Substitutions and Unification}\label{sec:substitutions-and-unification}
In Section~\ref{subsubsec:variable_assignment} we introduced the notation \(\sigma[x \mapsto d]\) to indicate a variable assignment in the semantic evaluation of a formula. We now extend this concept to the purely \emph{syntactic} setting, where variables are replaced by \emph{terms} rather than by elements of the interpretation domain. This leads to the notions of \emph{substitution}, \emph{unification}, and \emph{most general unifier}.

\subsection{Substitutions}\label{subsec:substitutions}
Given a term or literal \(\tau\), the notation \(\tau[x_k \mapsto t]\) denotes the expression obtained by replacing every occurrence of the variable \(x_k\) in \(\tau\) with the term \(t\).  
For example, if \(\tau = g(x_1, x_2)\), then:
\begin{equation}
\tau[x_1 \mapsto h] = g(h, x_2)
\end{equation}
A \bold{substitution} is a function mapping a subset of variables \(\mathcal{X}^{'} \subseteq \mathcal{X}\) to a set of terms \(\mathcal{T} \subseteq \mathcal{X} \cup \mathcal{F} \cup \mathcal{C}\):
\begin{equation}
\sigma = \{ x_1\mapsto t_1, \ldots, x_n\mapsto t_n \}
\end{equation}
If \(\tau\) is a term and \(\sigma\) is a substitution, we write \(\tau\sigma\) or \(\tau[x_1 \mapsto t_1, \ldots, x_n \mapsto t_n]\) for the term obtained by replacing all \(x_i\) with \(t_i\) \emph{simultaneously}.  
The word “simultaneously” means that each replacement is made with respect to the \emph{original} \(\tau\), without the effect of one replacement influencing another.

For instance, if \(\tau = f(x_1, x_2)\), then:
\begin{equation}
\tau[x_1 \mapsto x_2,  \ms  x_2 \mapsto x_1] = f(x_2, x_1)
\end{equation}
and not \(f(x_1, x_1)\), which would arise from sequential application.

\subsection{Generality of  Substitutions}\label{subsec:generality-of-substitutions}
For two substitutions \(\sigma_1\) and \(\sigma_2\), we say \(\sigma_1\) is \bold{more general} than \(\sigma_2\) if for every term \(\tau\) there exists a substitution \(\theta\) such that:
\begin{equation}  
  \tau\sigma_2 = (\tau\sigma_1)\theta
\end{equation}
If there is a substitution \(\sigma\) with:
\begin{equation}
\tau_1\sigma = \tau_2\sigma
\end{equation}
then \(\tau_1\) and \(\tau_2\) are said to be \bold{unifiable}, and \(\sigma\) is called a \bold{unifier}.
A unifier \(\sigma\) for two terms \(\tau_1\) and \(\tau_2\) is called the \bold{most general unifier} (MGU) if it is more general than any other unifier of \(\tau_1\) and \(\tau_2\).

\subsection{Unification Beyond Single Terms}\label{subsec:unification-beyond-single-terms}
The unification concept extends to sets of terms, literals, and sets of literals.  
A set of terms \(\mathcal{T}\) is \bold{unifiable} if there exists a substitution \(\sigma\) such that for any \(\tau_i, \tau_j \in \mathcal{T}\) we have \(\tau_i\sigma = \tau_j\sigma\). In this case, \(\sigma\) is a unifier of \(T\).

Two literals with the same arity:
\begin{equation}
L_1 = P(\tau_1, \ldots, \tau_n), \quad L_2 = \neg P(\tau'_1, \ldots, \tau'_n)
\end{equation}
are unifiable if there is a substitution making them identical, disregarding negation.
Literals of different arities are never unifiable.

\subsection{Obstructions to Unification}\label{subsec:obstructions-to-unification}
When attempting to unify two terms, two main forms of obstruction can occur:
\begin{enumerate}
    \item \bold{Function obstruction:} in a parallel traversal of the syntactic trees of both terms, two different function symbols occur at corresponding positions. For example:
    \[
    f(x_1, g_1(x_2)) \quad\text{and}\quad f(x_1, g_2(x_2))
    \]
    are not unifiable because \(g_1 \neq g_2\).
    \item \bold{Variable obstruction:} in a parallel traversal, a variable \(x\) appears in one term while the other term contains a subterm \(t \neq x\) in which \(x\) occurs. For example:
    \[
    h(x_1, k(x_2)) \quad\text{and}\quad h(k(x_1), x_1)
    \]
    are not unifiable because \(x_1\) occurs inside \(k(x_1)\).
\end{enumerate}

In general so, if two terms are not unifiable, then at least one of the following holds:
\begin{enumerate}
    \item The terms have different arities.
    \item The terms exhibit a function obstruction.
    \item The terms exhibit a variable obstruction.
\end{enumerate}


\subsection{Complexity of Unification}\label{subsec:complexity-of-unification}
The computational complexity of unification has been extensively studied since the 1960s.
Early approaches exhibited worst-case exponential behaviour due to the difficulty of detecting variable obstructions efficiently~\cite{robinson1965}.
However, significant theoretical advances in the 1970s led to the development of linear-time unification algorithms that achieve \(O(n)\) complexity, where \(n\) is the combined size of the input terms~\cite{martelli1976, paterson1978}.

These linear-time bounds represent optimal complexity for the unification problem, as any algorithm must examine all symbols in the input terms at least once.
Modern implementations incorporate various optimization techniques such as term indexing and specialized handling for common cases to improve average-case performance~\cite{martelli1976}.
The unification problem belongs to complexity class \bold{P}, making it tractable for practical applications in automated reasoning and logic programming systems.


\section{Skolemization and Normalization}\label{sec:skolemization_and_normalization}

Often times, First-Order formulae undergo a series of transformations to prepare them for automated reasoning tasks.
These transformations include renaming variables, converting to normal forms, and Skolemization.
Raw first-order logic formulae, as naturally expressed, are typically not in a form that automated theorem provers and inference engines can efficiently process.
The transformation process standardizes the syntactic structure of formulae, eliminates certain logical constructs that complicate automated reasoning, and converts them into canonical forms that enable the systematic application of inference rules and resolution procedures.

\subsection{Negated Normal Form}\label{subsec:negated_normal_form}
The first transformation applied is to convert the formula into \bold{negated normal form} (NNF).

A formula is in NNF if it is described by the following CFG\@:
\begin{equation}
  \nu \to \top  \ms | \ms  \bot  \ms | \ms  L  \ms | \ms  \nu \land \nu  \ms | \ms  \nu \lor \nu  \ms | \ms  \forall x  \ms  \nu  \ms | \ms  \exists x  \ms  \nu \quad\quad \text{where } L \text{ is a literal}
\end{equation}
In a practical sense, NNF transformation involves pushing negations inward and eliminating implications and biconditionals.
In fact, every formula can be brought into this form by replacing implications (\(\implies\)) and equivalences (\(\iff\)) by their definitions, using De Morgan's laws to push negation inwards, and eliminating double negations. This process can be represented using the following rewrite rules:

\begin{equation}
  \begin{aligned}
    \phi \implies \psi &\leadsto \neg \phi \lor \psi \\
    \phi \iff \psi &\leadsto (\neg \phi \lor \psi) \land (\phi \lor \neg \psi) \\
    \neg (\phi \lor \psi) &\leadsto \neg \phi \land \neg \psi \\
    \neg (\phi \land \psi) &\leadsto \neg \phi \lor \neg \psi \\
    \neg \neg \phi &\leadsto \phi \\
    \neg \exists x  \ms  \phi &\leadsto \forall x  \ms  \neg \phi \\
    \neg \forall x  \ms  \phi &\leadsto \exists x  \ms  \neg \phi
  \end{aligned}
\end{equation}
The NNF version of a formula is \emph{equivalent} to the original formula, meaning they have the same truth value in every model.

\subsection{Skolemization}\label{subsec:skolemization}
In the context of automated reasoning, \bold{Skolemization} is the process of eliminating existential quantifiers from a formula by introducing new symbols called \bold{Skolem functions} or \bold{Skolem constants}~\cite{skolem1920}.  
The result of Skolemization is not logically equivalent to the original formula but, relying on the axiom of choice, it is \bold{equisatisfiable}: the original formula is satisfiable if and only if the Skolemized version is satisfiable.
The process of Skolemization goes as follows:

Let \(\phi\) be a formula in NNF\@. For every existential quantifier \(\exists x_k\) in \(\phi\):
\begin{itemize}
  \item If \(x_k\) is not in the scope of any universal quantifier, replace it with a new constant symbol \(c\) (a \emph{Skolem constant}).
  \item If \(x_k\) is in the scope of universal quantifiers over variables \(y_1, \dots, y_m\), replace \(x_k\) with a new function symbol \(f(y_1, \dots, y_m)\) (a \emph{Skolem function}) of arity \(m\).
\end{itemize}
All Skolem symbols introduced must be fresh, i.e., not present in the original formulae.
An example of this process is as follows:

Consider the formula:
\[
\forall x \: \exists y \: P(x, y)
\]
Since \(y\) is existentially quantified within the scope of \(\forall x\), we introduce a function \(f(x)\):
\[
\forall x \: P(x, f(x))
\]

The Skolemization process fundamentally transforms the logical structure of a formula completely eliminating all existential quantifiers from it, replacing them with concrete function or constant symbols that capture the dependencies between variables.
As mentioned before though, this transformation preserves satisfiability, but it does not preserve validity in the reverse direction.
A formula that is valid before Skolemization may lose this property afterward, since the introduction of Skolem functions can create additional constraints that were not present in the original formula.
However, the resulting formula contains only universal quantifiers, which significantly simplifies the reasoning process by enabling purely universal reasoning techniques. 
This uniform quantifier structure is especially advantageous for automated theorem provers, as it eliminates the need to handle the complex interactions between universal and existential quantifiers% during proof search.

\subsection{Clausal Normal Form}\label{subsec:clausal_normal_form}
A formula is in CNF if it is a conjunction of one or more \bold{clauses}, each clause being a disjunction of literals.  
As NNF, CNF can be described by a CFG too:
\begin{equation}
  \begin{aligned}
    CNF &\to D  \ms | \ms  D \land CNF \\
      D &\to L  \ms | \ms  L \lor D
  \end{aligned}
  \qquad \text{where } L \text{ is a literal}
\end{equation}
Every NNF formula, after undergoing Skolemization, can be transformed into CNF by applying \(\land\) and \(\lor\) distribution rules and dropping universal quantifiers.
Often, CNF formulae are represented as sets of clauses with the following notation:
\[
(A \lor B) \land (C \lor D) \equiv \{\{A , B\} \ms , \ms \{C , D\}\}
\]

As an example of the full pipeline of the transformation from general formula to CNF, we consider the formula \( \forall x  \ms \big( \ms (\exists y  \ms   P(x,y)) \iff Q(x)  \ms  \big)\):

\begin{enumerate}
    \item \bold{Convert to NNF:}
    \begin{itemize}
        \item Eliminate \(\iff\):
          \[\forall x  \ms \big( (\neg \exists y_1 \ms  P(x,y_1) \lor Q(x)) \land (\neg Q(x) \lor \exists y_2 \ms  P(x,y_2))\big)\]
        \item Push negation inward using De Morgan's laws and quantifier duality:
    \end{itemize}
    \[
    \forall x \ms \Big(\big(\forall y \ms \neg P(x,y)\ \lor\ Q(x)\big)\ \land\ \big(\neg Q(x)\ \lor\ \exists y \ms  P(x,y)\big)\Big)
    \]
    
    
    \item \bold{Skolemization:}
    \begin{itemize}
        \item Replace \(\exists y\) (in scope of \(\forall x\)) with Skolem function \(f(x)\):
    \end{itemize}
    \[
      \forall x \ms \Big(\big(\forall y \ms \neg P(x,y)\ \lor\ Q(x)\big)\ \land\ \big(\neg Q(x)\ \lor\ P(x,f(x))\big)\Big)
    \]
    \\
    \item \bold{Convert to CNF:}
    \begin{itemize}
    \item Move all universal quantifiers to the front:
    \[
      \forall x \ms  \forall y \ms  \Big(\big(\neg P(x,y)\ \lor\ Q(x)\big)\ \land\ \big(\neg Q(x)\ \lor\ P(x,f(x))\big)\Big)
    \]
    \item Drop universal quantifiers and write as set of clauses:
    \[
      \{\{\neg P(x,y), Q(x)\}, \ms  \{\neg Q(x), P(x,f(x))\}\}
    \]
    \end{itemize}
    
\end{enumerate}

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
      scale=0.6, % <-- SCALING FACTOR
      transform shape, % <-- ENSURE NODES AND TEXT ARE SCALED
      node distance=2.2cm and 0.8cm,
      stage/.style={draw, rounded corners=3pt, fill=gray!10, align=center, inner sep=4pt, minimum height=2.8cm},
      >=Latex
    ]

    % Nodes arranged vertically for better readability
    \node[stage] (raw) {\bold{Raw Formula} \\[4pt]
      \(\displaystyle \forall x \ms \big( \ms (\exists y \ms  P(x,y)) \iff Q(x) \ms \big)\)};

    \node[stage, below=of raw] (nnf) {\bold{Negation Normal Form (NNF)}\\[4pt]
      \(\displaystyle \forall x \ms \Big(\big(\forall y \ms \neg P(x,y)\ \lor\ Q(x)\big) \land\ \big(\neg Q(x)\ \lor\ \exists y \ms  P(x,y)\big)\Big)\)};


    \node[stage, below=of nnf] (sko) {\bold{Skolemization}\\[4pt]
      \(\displaystyle \forall x \ms \Big(\big(\forall y \ms \neg P(x,y)\ \lor\ Q(x)\big)\ \land\ \big(\neg Q(x)\ \lor\ P(x,f(x))\big)\Big)\)};

    \node[stage, below=of sko] (cnf) {\bold{Clausal Normal Form (CNF)}\\[4pt]
      \(\{\{ \ms \neg P(x,y), \ms  Q(x) \ms \} \ms , \ms \{ \ms \neg Q(x), \ms  P(x,f(x)) \ms \}\}\)};

    % Arrows
    \draw[->] (raw) -- node[right, text width=3cm, align=left]{\small Eliminate\\biconditionals} (nnf);
    \draw[->] (nnf) -- node[right, text width=3cm, align=left]{\small Introduce Skolem\\functions} (sko);
    \draw[->] (sko) -- node[right, text width=3cm, align=left]{\small Convert to\\clause form} (cnf);

  \end{tikzpicture}
  \caption{Clausification: Raw formula \(\rightarrow\) NNF  \(\rightarrow\) Skolemization \(\rightarrow\) CNF.}\label{fig:nnf-to-cnf-pipeline}
\end{figure}


Transforming formulae into CNF is a prerequisite for many automated reasoning techniques, particularly \emph{resolution}~\cite{chang1997}.  
By ensuring that the input is in a uniform syntactic format, inference rules can be applied in a purely mechanical way without further structural transformations.

This pipeline of transformations, combined with \emph{naming}, is also called \bold{clausification} and  constitutes the \bold{preprocessing} phase of automated reasoning systems, a crucial step to prepare formulae for \emph{inferences}.

\section{Auxiliary Predicate Introduction}\label{sec:auxiliary_predicate_introduction}

In the transformation of first-order logic (FOL) formulas, particularly when converting to clausal form, it is often advantageous to introduce \emph{auxiliary predicates} to represent complex subformulae. The general idea is to replace a subformula \(\psi(\bar{x})\) with\footnote{Here the notation \(\bar{x}\) is used as usual to indicate a sequence of variables, while the notation \(\psi(\bar{x})\) indicates a formula with \(\bar{x}\) as free variables.} a fresh predicate symbol \(P(\bar{x})\) that does not occur in the original vocabulary, and to accompany this replacement with a \emph{defining axiom} of the form
\[
\forall \bar{x}\ \big(P(\bar{x}) \iff \psi(\bar{x})\big).
\]
This method is referred to in the literature under various names, including \emph{auxiliary predicate introduction}, \emph{renaming}, and, in some contexts, \emph{axiomatization}. The defining axiom ensures that \(P\) is logically equivalent to the original subformula, thereby preserving satisfiability and logical consequence.

One of the main motivations for this technique in automated theorem proving is the avoidance of an exponential increase in formula size during \emph{clausification}.
A naïve elimination of biconditionals and the distribution of disjunctions over conjunctions can lead to a number of clauses that grows exponentially with the size of the input.
By introducing auxiliary predicates for large or deeply nested subformulae, the number of resulting clauses can be kept proportional to the size of the original formula.

Another important application arises in the context of syntactically restricted fragments of FOL, such as the \emph{guarded fragment} or the \emph{fluted fragment}.
In these fragments, unrestricted formula transformations may produce formulas that fall outside the fragment, thereby forfeiting desirable properties such as decidability or favourable complexity bounds.
Here, auxiliary predicate introduction serves as a \emph{structure-preserving} transformation: rather than expanding a subformula in a way that would violate syntactic restrictions, a fresh predicate is introduced, with a defining axiom that maintains semantic equivalence while preserving the fragment membership of the formula.

\subsection{Polarity of Subformulae}\label{subsec:polarity-of-subformulae}

The concept of \emph{polarity} provides a finer-grained analysis of where and how a subformula occurs within a larger formula.
Intuitively, polarity describes whether an occurrence of a subformula is in a \emph{positive}, \emph{negative}, or \emph{neutral} position.
A subformula occurs positively if it is under the scope of an even number of negations, and negatively if it is under the scope of an odd number of negations.
The \emph{neutral} (or \emph{zero}) polarity occurs in positions where a subformula is required in both directions of entailment, most notably when it appears under a biconditional, where neither purely positive nor purely negative reasoning suffices.

Polarity is defined inductively as follows:
\[
\begin{aligned}
&\text{(Base case)} && \mathrm{pol}(\varphi) = + \quad \text{if } \varphi \text{ is the whole formula}, \\[2pt]
&\text{(Negation)} && \mathrm{pol}(\neg \varphi) = -\mathrm{pol}(\varphi), \\[2pt]
&\text{(Conjunction / Disjunction)} && 
\begin{cases}
\mathrm{pol}(\varphi) = \mathrm{pol}(\varphi \landor \psi), \\
\mathrm{pol}(\psi) = \mathrm{pol}(\varphi \landor \psi),
\end{cases} \\[4pt]
&\text{(Implication)} && 
\begin{cases}
\mathrm{pol}(\varphi) = -\mathrm{pol}(\varphi \rightarrow \psi), \\
\mathrm{pol}(\psi) = \mathrm{pol}(\varphi \rightarrow \psi),
\end{cases} \\[4pt]
&\text{(Biconditional)} && 
\begin{cases}
\mathrm{pol}(\varphi) = 0, \\
\mathrm{pol}(\psi) = 0,
\end{cases} \quad \text{for } \varphi \leftrightarrow \psi \text{ with any polarity}, \\[4pt]
&\text{(Quantifiers)} && \mathrm{pol}(\varphi) = \mathrm{pol}(\mathcal{Q} x\, \varphi), \quad \mathcal{Q} \in \{\forall, \exists\}.
\end{aligned}
\]
Here, \(+\) denotes positive polarity, \(-\) denotes negative polarity, and \(0\) denotes neutral polarity.



\subsection{Polarity-Aware Auxiliary Predicate Introduction}\label{subsec:polarity-aware-auxiliary-predicate-introduction}

Polarity analysis allows for an optimization of auxiliary predicate introduction. If a subformula \(\psi(\bar{x})\) occurs only positively in the formula, it is sufficient to introduce the fresh predicate \(P(\bar{x})\) together with the implication
\[
\forall \bar{x}\ \big(P(\bar{x}) \implies \psi(\bar{x})\big),
\]
rather than the full biconditional. Conversely, if \(\psi(\bar{x})\) occurs only negatively, it suffices to add
\[
\forall \bar{x}\ \big(\psi(\bar{x}) \implies P(\bar{x})\big).
\]
The full biconditional is necessary only when \(\psi(\bar{x})\) occurs with zero polarity.

This optimization is widely used in \emph{definitional CNF transformations} for first-order theorem proving. By introducing only the necessary direction of the equivalence, the number of clauses generated during clausification can be reduced, often substantially, while still preserving the correctness of the transformation with respect to satisfiability and logical consequence.







